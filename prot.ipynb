{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/23 11:41:17 WARN Utils: Your hostname, DESKTOP-C98VC8U resolves to a loopback address: 127.0.1.1; using 172.23.181.208 instead (on interface eth0)\n",
      "21/08/23 11:41:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/23 11:41:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "customers = spark.read.parquet(\"./output/customers.parquet\")\n",
    "customers.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|               1|    Terri| Leonard|          3124065463|   65369|          Mrs.|\n",
      "|               2|  Maurice|    West|001-468-075-5301x308|   27061|           Dr.|\n",
      "|               3|   Taylor|    Hall|    001-004-938-2651|   88302|           Mr.|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tickets = spark.read.parquet(\"./output/tickets.parquet\")\n",
    "tickets.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|ticket_id|order_id|customer_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|        1|     182|        160|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        2|     376|         46|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        3|     282|        116|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "tickets.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- ticket_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- Net_sales: double (nullable = true)\n",
      " |-- event_code: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_season: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#broadcast joining because one table is very small, reduces shuffling\n",
    "import pyspark.sql.functions as F\n",
    "join_condition = customers.CustomerIdentity == tickets.customer_id\n",
    "combined_df =  tickets.join(F.broadcast(customers), join_condition).drop(\"CustomerIdentity\")\n",
    "combined_df.cache()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ticket_id: int, order_id: int, customer_id: int, quantity: int, Net_sales: double, event_code: string, event_name: string, event_season: string, Date: date, FirstName: string, LASTNAME: string, phone: string, postcode: string, Customer_Title: string]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "combined_df.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "combined_df.select(\"Date\").where(F.col(\"Date\").isNull()).count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "combined_df.show(5, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+----------------------+--------+--------------+\n",
      "|ticket_id|order_id|customer_id|quantity|Net_sales|event_code|event_name        |event_season|Date      |FirstName|LASTNAME|phone                 |postcode|Customer_Title|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+----------------------+--------+--------------+\n",
      "|1        |182     |160        |1       |58.55    |CHL-ARS   |Chelsea vs Arsenal|2020/2021   |2018-05-05|Tony     |Baird   |001-097-596-0629x46835|94414   |Ms.           |\n",
      "|2        |376     |46         |1       |44.25    |CHL-ARS   |Chelsea vs Arsenal|2020/2021   |2018-05-05|Justin   |Higgins |1889149702            |67579   |Mrs.          |\n",
      "|3        |282     |116        |5       |47.0     |CHL-ARS   |Chelsea vs Arsenal|2020/2021   |2018-05-05|Jennifer |Dawson  |479.687.6222x409      |23815   |Dr.           |\n",
      "|4        |20      |76         |2       |29.13    |BTH-SRC   |Bath vs Saracens  |2020/2021   |2018-01-30|Jasmine  |Bryan   |050.499.1449          |50492   |Mrs.          |\n",
      "|5        |14      |190        |5       |4.15     |BTH-SRC   |Bath vs Saracens  |2020/2021   |2018-01-30|Larry    |Rivera  |001-966-104-7383x0688 |84129   |Dr.           |\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+----------------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "combined_df.select(\"customer_id\").distinct().count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "combined_df.createOrReplaceTempView(\"DF\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT order_id, Date \n",
    "          from DF \n",
    "          where Date == \"2018-01-30\"\n",
    "          \"\"\").show(2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------+\n",
      "|order_id|      Date|\n",
      "+--------+----------+\n",
      "|      20|2018-01-30|\n",
      "|      14|2018-01-30|\n",
      "+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "spark.sql(\"\"\"\n",
    "          drop database trial\n",
    "          \"\"\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "TRANSACTION = combined_df.select(\"order_id\", \"Date\", \"customer_id\", \"quantity\", \"Net_sales\").distinct().orderBy(\"Date\")\n",
    "TRANSACTION.count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "TRANSACTION.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------+-----------+--------+---------+\n",
      "|order_id|      Date|customer_id|quantity|Net_sales|\n",
      "+--------+----------+-----------+--------+---------+\n",
      "|     359|2018-01-30|         47|       2|    49.32|\n",
      "|     182|2018-01-30|        150|       2|    94.19|\n",
      "|     192|2018-01-30|         69|       5|    84.86|\n",
      "|      76|2018-01-30|         68|       5|    81.74|\n",
      "|      98|2018-01-30|         10|       4|    75.13|\n",
      "+--------+----------+-----------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "EVENT = combined_df.select(\"event_name\", \"event_code\", \"event_season\").distinct().show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+------------------+----------+------------+\n",
      "|order_id|        event_name|event_code|event_season|\n",
      "+--------+------------------+----------+------------+\n",
      "|      34|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|     348|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     237|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|      41|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     205|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     108|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|      83|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|      62|Chelsea vs Arsenal|   CHL-ARS|   2020/2021|\n",
      "|      13|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     388|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|      73|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     293|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     200|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|     227|Chelsea vs Arsenal|   CHL-ARS|   2020/2021|\n",
      "|     392|  Bath vs Saracens|   BTH-SRC|   2020/2021|\n",
      "|     378|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|      11|Chelsea vs Arsenal|   CHL-ARS|   2020/2021|\n",
      "|     311|Chelsea vs Arsenal|   CHL-ARS|   2020/2021|\n",
      "|     107|       Silverstone|   F1-SILV|   2020/2021|\n",
      "|     360|Chelsea vs Arsenal|   CHL-ARS|   2020/2021|\n",
      "+--------+------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/24 08:05:39 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 48681181 ms exceeds timeout 120000 ms\n",
      "21/08/24 08:05:39 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "CUSTOMER = combined_df.select(\"customer_id\", \"Customer_Title\", \"FirstName\", \"LASTNAME\", \"phone\", \"postcode\").distinct().orderBy(\"customer_id\")\n",
    "CUSTOMER.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------+-----------+--------+--------------------+--------+\n",
      "|customer_id|Customer_Title|  FirstName|LASTNAME|               phone|postcode|\n",
      "+-----------+--------------+-----------+--------+--------------------+--------+\n",
      "|          1|          Mrs.|      Terri| Leonard|          3124065463|   65369|\n",
      "|          2|           Dr.|    Maurice|    West|001-468-075-5301x308|   27061|\n",
      "|          3|           Mr.|     Taylor|    Hall|    001-004-938-2651|   88302|\n",
      "|          4|           Dr.|Christopher|  Mosley|          9539855170|   23959|\n",
      "|          5|          Miss|     Lauren|   Davis|       (559)098-0114|   79819|\n",
      "|          6|           Dr.|     Steven|  Norris|       (160)936-9915|   27807|\n",
      "|          7|           Mr.|      Randy|    Ross|    267.888.4934x589|   85772|\n",
      "|          8|           Mr.|   Victoria|Thompson|        534-302-6739|   83797|\n",
      "|          9|          Mrs.|      Lance|   Moore|          1090466007|   27443|\n",
      "|         10|           Mr.|     Ashlee|   Smith|          1406857107|   45034|\n",
      "|         11|           Dr.|      Kevin|  Morrow|001-855-335-5372x701|   71735|\n",
      "|         12|          Mrs.|    Stephen|   Ortiz|001-165-828-1070x...|   09009|\n",
      "|         13|          Mrs.|     Connie| Ramirez|        738-770-5192|   61168|\n",
      "|         14|           Dr.|        Jay|   Hicks| (555)199-5192x85056|   36788|\n",
      "|         15|          Miss|      Larry|  Turner|   612.318.7818x4011|   96540|\n",
      "|         16|           Ms.|      Tyler|    Ward|        741-591-4867|   87900|\n",
      "|         17|           Mr.|       John|  Knight|001-609-488-6748x599|   71360|\n",
      "|         18|           Mr.|      Julia| Ramirez|001-618-880-4522x...|   59781|\n",
      "|         19|           Mr.|     Joseph| Swanson|   (367)594-7987x430|   35067|\n",
      "|         20|           Mr.|      Jason|   Ellis|        710.776.3960|   44478|\n",
      "+-----------+--------------+-----------+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import pyspark.sql.functions as F\n",
    "# 3. For each Customer, a list of Events\n",
    "Edf = combined_df.groupBy(\"customer_id\")\\\n",
    "                 .agg(F.collect_list(F.col(\"event_code\"))\\\n",
    "                 .alias(\"Events_list\"))\n",
    "Edf.show(5, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|customer_id|Events_list                                                                               |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|148        |[BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV, CHL-ARS]                                             |\n",
      "|31         |[CHL-ARS, CHL-ARS, BTH-SRC, BTH-SRC, CHL-ARS, CHL-ARS, BTH-SRC, CHL-ARS, BTH-SRC, CHL-ARS]|\n",
      "|85         |[CHL-ARS, F1-SILV, F1-SILV]                                                               |\n",
      "|137        |[BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV, BTH-SRC, CHL-ARS, BTH-SRC]                           |\n",
      "|65         |[F1-SILV, BTH-SRC, CHL-ARS]                                                               |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# combined_df.groupBy(\"CustomerIdentity\").agg(F.count(\"event_code\").alias(\"Totals\")).sort(\"Totals\", ascending=True).show()\n",
    "Tots = combined_df.groupBy(\"customer_id\").agg(F.count(\"event_name\").alias(\"Totals\"))\n",
    "Tots.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+\n",
      "|customer_id|Totals|\n",
      "+-----------+------+\n",
      "|        148|     5|\n",
      "|         31|    10|\n",
      "|        137|     7|\n",
      "|         85|     3|\n",
      "|         65|     3|\n",
      "|         53|     1|\n",
      "|        133|     9|\n",
      "|         78|     3|\n",
      "|        108|    11|\n",
      "|        155|     4|\n",
      "|         34|     8|\n",
      "|        193|     6|\n",
      "|        115|     4|\n",
      "|        101|     5|\n",
      "|        126|     1|\n",
      "|         81|     2|\n",
      "|        183|     9|\n",
      "|         28|     1|\n",
      "|         76|    10|\n",
      "|         26|     4|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "MultiEvent = Tots.withColumn(\"MultiEvent\", F.col(\"Totals\") > 1)\n",
    "MultiEvent.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+----------+\n",
      "|customer_id|Totals|MultiEvent|\n",
      "+-----------+------+----------+\n",
      "|        148|     5|      true|\n",
      "|         31|    10|      true|\n",
      "|        137|     7|      true|\n",
      "|         85|     3|      true|\n",
      "|         65|     3|      true|\n",
      "|         53|     1|     false|\n",
      "|        133|     9|      true|\n",
      "|         78|     3|      true|\n",
      "|        108|    11|      true|\n",
      "|        155|     4|      true|\n",
      "|         34|     8|      true|\n",
      "|        193|     6|      true|\n",
      "|        115|     4|      true|\n",
      "|        101|     5|      true|\n",
      "|        126|     1|     false|\n",
      "|         81|     2|      true|\n",
      "|        183|     9|      true|\n",
      "|         28|     1|     false|\n",
      "|         76|    10|      true|\n",
      "|         26|     4|      true|\n",
      "+-----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "multiEvent1 = lambda x: \"true\" if x>1 else \" \""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def multiEvent(x):\n",
    "    \n",
    "   if x > 1:\n",
    "\n",
    "        return   \"True\"\n",
    "   else:\n",
    "        return \" \"\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "multiEvent1_udf = udf(multiEvent1, StringType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "Tots.withColumn(\"MultiEvent\", multiEvent_udf(F.col(\"Totals\"))).show(5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "MultiEvent123 = Tots.withColumn(\"MultiEvent88\", multiEvent1_udf(F.col(\"Totals\")))\n",
    "MultiEvent123.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+------------+\n",
      "|customer_id|Totals|MultiEvent88|\n",
      "+-----------+------+------------+\n",
      "|        148|     5|        true|\n",
      "|         31|    10|        true|\n",
      "|        137|     7|        true|\n",
      "|         85|     3|        true|\n",
      "|         65|     3|        true|\n",
      "|         53|     1|            |\n",
      "|        133|     9|        true|\n",
      "|         78|     3|        true|\n",
      "|        108|    11|        true|\n",
      "|        155|     4|        true|\n",
      "|         34|     8|        true|\n",
      "|        193|     6|        true|\n",
      "|        115|     4|        true|\n",
      "|        101|     5|        true|\n",
      "|        126|     1|            |\n",
      "|         81|     2|        true|\n",
      "|        183|     9|        true|\n",
      "|         28|     1|            |\n",
      "|         76|    10|        true|\n",
      "|         26|     4|        true|\n",
      "+-----------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "cols = [\"customer_id\", \"event_name\", \"event_season\", \"Date\", \"rank\"]\n",
    "combined_df.groupBy(\"customer_id\").agg(F.max(\"quantity\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|max(quantity)|\n",
      "+-----------+-------------+\n",
      "|        148|            4|\n",
      "|         31|            5|\n",
      "|        137|            5|\n",
      "|         85|            5|\n",
      "|         65|            4|\n",
      "|         53|            2|\n",
      "|        133|            5|\n",
      "|         78|            4|\n",
      "|        108|            5|\n",
      "|        155|            5|\n",
      "|         34|            4|\n",
      "|        193|            5|\n",
      "|        115|            5|\n",
      "|        101|            5|\n",
      "|        126|            5|\n",
      "|         81|            3|\n",
      "|        183|            5|\n",
      "|         28|            3|\n",
      "|         76|            5|\n",
      "|         26|            5|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# 5. Largest Order by Quantity for each Customer\n",
    "from pyspark.sql.window import Window\n",
    "cols = [\"customer_id\", \"quantity\"]\n",
    "windowSpec1 = Window.partitionBy([\"customer_id\"])\\\n",
    "                    .orderBy(F.desc(\"quantity\"))\n",
    "\n",
    "combined_df.withColumn(\"rank\", F.rank().over(windowSpec1)).filter(\"rank == 1\").dropDuplicates().select(cols).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|quantity|\n",
      "+-----------+--------+\n",
      "|        148|       4|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "|         85|       5|\n",
      "|        137|       5|\n",
      "|        137|       5|\n",
      "|         65|       4|\n",
      "|         53|       2|\n",
      "|        133|       5|\n",
      "|        133|       5|\n",
      "|        133|       5|\n",
      "|         78|       4|\n",
      "|         78|       4|\n",
      "|        108|       5|\n",
      "|        108|       5|\n",
      "|        108|       5|\n",
      "|        155|       5|\n",
      "|         34|       4|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"quantity\"))\n",
    "\n",
    "F = combined_df.withColumn(\"dddd\", F.collect_list(F.col(\"quantity\")).over(WindowSpec)).select(\"customer_id\", \"dddd\")\n",
    "F.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|                dddd|\n",
      "+-----------+--------------------+\n",
      "|        148|                 [4]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|     [4, 3, 3, 3, 2]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|  [5, 5, 5, 5, 4, 4]|\n",
      "|         31|  [5, 5, 5, 5, 4, 4]|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         85|                 [5]|\n",
      "|         85|              [5, 4]|\n",
      "|         85|           [5, 4, 1]|\n",
      "|        137|              [5, 5]|\n",
      "|        137|              [5, 5]|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "F.collect()[2][\"dddd\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"customer_id\")\n",
    "\n",
    "df22 = combined_df.withColumn(\"EventsList\", \n",
    "                              F.collect_list(F.col(\"event_code\"))\n",
    "                              .over(WindowSpec)\n",
    "                              )\\\n",
    ".select(\"customer_id\", \"EventsList\")\n",
    "df22.show(5, truncate=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+---------------------------------------------+\n",
      "|customer_id|EventsList                                   |\n",
      "+-----------+---------------------------------------------+\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "+-----------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"event_code\", outputCol=\"IndexEvents\")\n",
    "indexed = indexer.fit(combined_df).transform(combined_df)\n",
    "indexed.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|ticket_id|order_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|IndexEvents|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|        1|     182|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             160|     Tony|   Baird|001-097-596-0629x...|   94414|           Ms.|        0.0|\n",
      "|        2|     376|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              46|   Justin| Higgins|          1889149702|   67579|          Mrs.|        0.0|\n",
      "|        3|     282|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             116| Jennifer|  Dawson|    479.687.6222x409|   23815|           Dr.|        0.0|\n",
      "|        4|      20|       2|    29.13|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              76|  Jasmine|   Bryan|        050.499.1449|   50492|          Mrs.|        1.0|\n",
      "|        5|      14|       5|     4.15|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|             190|    Larry|  Rivera|001-966-104-7383x...|   84129|           Dr.|        1.0|\n",
      "|        6|     180|       5|    66.67|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|              50|  Richard|    Wade|        867-772-7090|   38681|          Mrs.|        2.0|\n",
      "|        7|      27|       2|    63.19|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|             119|   Joseph|   Avery|          3809985291|   56074|           Dr.|        2.0|\n",
      "|        8|     328|       3|    95.11|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              35|    Allen|   Glass|001-327-121-8445x...|   34120|           Dr.|        1.0|\n",
      "|        9|     211|       5|     19.2|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              61|  Vincent| Sanchez|+1-098-920-7256x1...|   07205|           Dr.|        1.0|\n",
      "|       10|     301|       1|    13.42|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              12|  Stephen|   Ortiz|001-165-828-1070x...|   09009|          Mrs.|        0.0|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "combined_df.groupBy(\"Customer_Title\").agg(F.sum(\"quantity\").alias(\"Totals\")).sort(\"Totals\", ascending=False).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+------+\n",
      "|Customer_Title|Totals|\n",
      "+--------------+------+\n",
      "|           Dr.|  1023|\n",
      "|           Mr.|   996|\n",
      "|          Mrs.|   691|\n",
      "|          Miss|   131|\n",
      "|           Ms.|   128|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "add = lambda x,y : x+y\n",
    "add(2,3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "combined_df.groupBy(\"customer_id\").agg(F.count(\"event_code\").alias(\"Totals\")).sort(\"Totals\", ascending=True).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+\n",
      "|customer_id|Totals|\n",
      "+-----------+------+\n",
      "|         53|     1|\n",
      "|         86|     1|\n",
      "|        121|     1|\n",
      "|        126|     1|\n",
      "|        175|     1|\n",
      "|         28|     1|\n",
      "|        149|     1|\n",
      "|        145|     2|\n",
      "|         15|     2|\n",
      "|        156|     2|\n",
      "|          5|     2|\n",
      "|         61|     2|\n",
      "|         24|     2|\n",
      "|         62|     2|\n",
      "|          1|     2|\n",
      "|        117|     2|\n",
      "|        171|     2|\n",
      "|         13|     2|\n",
      "|         81|     2|\n",
      "|         91|     2|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c572b0966902eb897d15fe5d929823b06d400a3cf7da46c31443f305cdaa6b64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}