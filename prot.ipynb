{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "import logging\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/09/28 14:02:06 WARN Utils: Your hostname, DESKTOP-C98VC8U resolves to a loopback address: 127.0.1.1; using 172.22.212.21 instead (on interface eth0)\n",
      "21/09/28 14:02:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Ivy Default Cache set to: /home/wellington/.ivy2/cache\n",
      "The jars for the packages stored in: /home/wellington/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-822eb392-45f7-43a6-9b18-68a1eaeed747;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      ":: resolution report :: resolve 1198ms :: artifacts dl 63ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-822eb392-45f7-43a6-9b18-68a1eaeed747\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/19ms)\n",
      "21/09/28 14:02:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/28 14:02:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/09/28 14:02:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pyspark.sql.types import *\n",
    "schema1 = StructType([\n",
    "                        StructField(\"ticket_id\", IntegerType(), True),\n",
    "                        StructField(\"order_id\", IntegerType(), True),\n",
    "                        StructField(\"customer_id\", IntegerType(), True),\n",
    "                        StructField(\"quantity\", IntegerType(), True),\n",
    "                        StructField(\"net_sales\", FloatType(), True),\n",
    "                        StructField(\"event_code\", StringType(), True),\n",
    "                        StructField(\"event_name\", StringType(), True),\n",
    "                        StructField(\"Event_date\", DateType(), True),\n",
    "                        StructField(\"event_season\", StringType(), True)]\n",
    "                        \n",
    "                    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "deltaTable = DeltaTable.convertToDelta(spark, \"parquet.`/mnt/c/DataSets/tickets.parquet`\") "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The table you are trying to convert is already a delta table\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df = spark.read.format(\"delta\").load(\"/mnt/c/DataSets/tickets.parquet\")\n",
    "df.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+------------------+----------+------------------+----------+------------+----------+\n",
      "|ticket_id|order_id|customer_id|quantity|         net_sales|event_code|        event_name|event_date|event_season|      Date|\n",
      "+---------+--------+-----------+--------+------------------+----------+------------------+----------+------------+----------+\n",
      "|        1|     182|        160|       1|58.549109713360856|   CHL-ARS|Chelsea vs Arsenal|5-May-2018|   2020/2021|2018-05-05|\n",
      "|        2|     376|         46|       1|44.254019825925596|   CHL-ARS|Chelsea vs Arsenal|5-May-2018|   2020/2021|2018-05-05|\n",
      "|        3|     282|        116|       5|47.003734818026274|   CHL-ARS|Chelsea vs Arsenal|5-May-2018|   2020/2021|2018-05-05|\n",
      "+---------+--------+-----------+--------+------------------+----------+------------------+----------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "cust1 = spark.read\\\n",
    "             .option(\"dateFormat\", \"dd-MMM-yyyy\")\\\n",
    "             .option(\"sep\", \"|\")\\\n",
    "             .option(\"header\", True)\\\n",
    "             .schema(schema1)\\\n",
    "             .csv(\"./data/tickets.csv\")\n",
    "  \n",
    "cust1.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8ANTLR Tool version 4.7 used for code generation does not match the current runtime version 4.8"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "|ticket_id|order_id|customer_id|quantity|net_sales|event_code|        event_name|Event_date|event_season|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "|        1|     182|        160|       1| 58.54911|   CHL-ARS|Chelsea vs Arsenal|2018-05-05|   2020/2021|\n",
      "|        2|     376|         46|       1| 44.25402|   CHL-ARS|Chelsea vs Arsenal|2018-05-05|   2020/2021|\n",
      "|        3|     282|        116|       5|47.003735|   CHL-ARS|Chelsea vs Arsenal|2018-05-05|   2020/2021|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "cust2 = spark.read\\\n",
    "             .option(\"dateFormat\", \"yyyy-MMM-dd\")\\\n",
    "             .option(\"sep\", \"|\")\\\n",
    "             .option(\"header\", True)\\\n",
    "             .schema(schema1)\\\n",
    "             .csv(\"./data/tickets.csv\")\n",
    "cust2.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "|ticket_id|order_id|customer_id|quantity|net_sales|event_code|        event_name|Event_date|event_season|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "|        1|     182|        160|       1| 58.54911|   CHL-ARS|Chelsea vs Arsenal|0010-11-08|   2020/2021|\n",
      "|        2|     376|         46|       1| 44.25402|   CHL-ARS|Chelsea vs Arsenal|0010-11-08|   2020/2021|\n",
      "|        3|     282|        116|       5|47.003735|   CHL-ARS|Chelsea vs Arsenal|0010-11-08|   2020/2021|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pyspark.sql.functions as F\n",
    "print(cust1.select(\"Event_date\").where(F.col(\"Event_date\").isNotNull()).count(), \n",
    "      cust2.select(\"Event_date\").where(F.col(\"Event_date\").isNotNull()).count()\n",
    "      )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "667 667\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cust1.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- ticket_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- net_sales: float (nullable = true)\n",
      " |-- event_code: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- Event_date: date (nullable = true)\n",
      " |-- event_season: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "customers = spark.read.parquet(\"./output/customers.parquet\")\n",
    "customers.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|               1|    Terri| Leonard|          3124065463|   65369|          Mrs.|\n",
      "|               2|  Maurice|    West|001-468-075-5301x308|   27061|           Dr.|\n",
      "|               3|   Taylor|    Hall|    001-004-938-2651|   88302|           Mr.|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tickets = spark.read.parquet(\"./output/tickets.parquet\")\n",
    "tickets.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|ticket_id|order_id|customer_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|        1|     182|        160|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        2|     376|         46|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        3|     282|        116|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tickets.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- ticket_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- Net_sales: double (nullable = true)\n",
      " |-- event_code: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_season: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#broadcast joining because one table is very small, reduces shuffling\n",
    "import pyspark.sql.functions as F\n",
    "join_condition = customers.CustomerIdentity == tickets.customer_id\n",
    "combined_df =  tickets.join(F.broadcast(customers), join_condition).drop(\"CustomerIdentity\")\n",
    "combined_df.cache()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[ticket_id: int, order_id: int, customer_id: int, quantity: int, Net_sales: double, event_code: string, event_name: string, event_season: string, Date: date, FirstName: string, LASTNAME: string, phone: string, postcode: string, Customer_Title: string]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(len(combined_df.columns), combined_df.count())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14 1000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "combined_df.select(\"Date\").where(F.col(\"Date\").isNull()).count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "combined_df.show(3, truncate=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+--------------------+--------+--------------+\n",
      "|ticket_id|order_id|customer_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|FirstName|LASTNAME|               phone|postcode|Customer_Title|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+--------------------+--------+--------------+\n",
      "|        1|     182|        160|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|     Tony|   Baird|001-097-596-0629x...|   94414|           Ms.|\n",
      "|        2|     376|         46|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|   Justin| Higgins|          1889149702|   67579|          Mrs.|\n",
      "|        3|     282|        116|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05| Jennifer|  Dawson|    479.687.6222x409|   23815|           Dr.|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+---------+--------+--------------------+--------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "combined_df.select(\"customer_id\").distinct().count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "combined_df.createOrReplaceTempView(\"DF\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "spark.sql(\"\"\"\n",
    "          SELECT Date, order_id\n",
    "          FROM DF \n",
    "          WHERE Date == \"2018-01-30\"\n",
    "          \"\"\").show(2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+--------+\n",
      "|      Date|order_id|\n",
      "+----------+--------+\n",
      "|2018-01-30|      20|\n",
      "|2018-01-30|      14|\n",
      "+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "cols1 = ['order_id', 'ticket_id', 'customer_id', 'event_code', 'Date',  'quantity', 'Net_sales']\n",
    "transcation = combined_df.select(cols1)\\\n",
    "                         .distinct()\\\n",
    "                         .orderBy(\"Date\")\n",
    "transcation.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+---------+-----------+----------+----------+--------+---------+\n",
      "|order_id|ticket_id|customer_id|event_code|      Date|quantity|Net_sales|\n",
      "+--------+---------+-----------+----------+----------+--------+---------+\n",
      "|     219|      188|        132|   F1-SILV|2018-01-30|       2|    28.82|\n",
      "|     342|      207|        180|   F1-SILV|2018-01-30|       5|    25.15|\n",
      "|       2|      838|         95|   BTH-SRC|2018-01-30|       2|     84.0|\n",
      "+--------+---------+-----------+----------+----------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "event = combined_df.select(\"event_code\", \"event_name\", \"event_season\").distinct()\n",
    "event.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------------------+------------+\n",
      "|event_code|        event_name|event_season|\n",
      "+----------+------------------+------------+\n",
      "|   BTH-SRC|  Bath vs Saracens|   2020/2021|\n",
      "|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|\n",
      "|   F1-SILV|       Silverstone|   2020/2021|\n",
      "+----------+------------------+------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "customer = combined_df.select(\"customer_id\", \"Customer_Title\", \"FirstName\", \"LASTNAME\", \"phone\", \"postcode\").distinct().orderBy(\"customer_id\")\n",
    "customer.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------+---------+--------+--------------------+--------+\n",
      "|customer_id|Customer_Title|FirstName|LASTNAME|               phone|postcode|\n",
      "+-----------+--------------+---------+--------+--------------------+--------+\n",
      "|          1|          Mrs.|    Terri| Leonard|          3124065463|   65369|\n",
      "|          2|           Dr.|  Maurice|    West|001-468-075-5301x308|   27061|\n",
      "|          3|           Mr.|   Taylor|    Hall|    001-004-938-2651|   88302|\n",
      "+-----------+--------------+---------+--------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import pyspark.sql.functions as F\n",
    "# 3. For each Customer, a list of Events\n",
    "Edf = combined_df.groupBy(\"customer_id\")\\\n",
    "                 .agg(F.collect_list(F.col(\"event_code\"))\\\n",
    "                 .alias(\"Events_list\"))\n",
    "Edf.show(3, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|customer_id|Events_list                                                                               |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "|148        |[BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV, CHL-ARS]                                             |\n",
      "|31         |[CHL-ARS, CHL-ARS, BTH-SRC, BTH-SRC, CHL-ARS, CHL-ARS, BTH-SRC, CHL-ARS, BTH-SRC, CHL-ARS]|\n",
      "|85         |[CHL-ARS, F1-SILV, F1-SILV]                                                               |\n",
      "+-----------+------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# combined_df.groupBy(\"CustomerIdentity\").agg(F.count(\"event_code\").alias(\"Totals\")).sort(\"Totals\", ascending=True).show()\n",
    "Tots = combined_df.groupBy(\"customer_id\").agg(F.count(\"event_name\").alias(\"Totals\"))\n",
    "Tots.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+\n",
      "|customer_id|Totals|\n",
      "+-----------+------+\n",
      "|        148|     5|\n",
      "|         31|    10|\n",
      "|        137|     7|\n",
      "+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "MultiEvent = Tots.withColumn(\"MultiEvent\", F.col(\"Totals\") > 1).drop(\"Totals\")\n",
    "MultiEvent.show(6)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|MultiEvent|\n",
      "+-----------+----------+\n",
      "|        148|      true|\n",
      "|         31|      true|\n",
      "|        137|      true|\n",
      "|         85|      true|\n",
      "|         65|      true|\n",
      "|         53|     false|\n",
      "+-----------+----------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "dfff = combined_df.select(\"Date\", \"customer_id\", \"ticket_id\", \"Net_sales\")\n",
    "dfff.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+---------+---------+\n",
      "|      Date|customer_id|ticket_id|Net_sales|\n",
      "+----------+-----------+---------+---------+\n",
      "|2018-05-05|        160|        1|    58.55|\n",
      "|2018-05-05|         46|        2|    44.25|\n",
      "|2018-05-05|        116|        3|     47.0|\n",
      "+----------+-----------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "from pyspark.sql.window import Window\n",
    "cols = [\"Date\", \"customer_id\", \"ticket_id\", \"quantity\", \"Net_sales\"]\n",
    "\n",
    "windowSpec = Window.partitionBy([\"customer_id\"])\\\n",
    "                    .orderBy(\"Date\")\\\n",
    "                    .rowsBetween(-1, Window.currentRow)\n",
    "res = combined_df.select(cols).withColumn(\"RunningTotal\", F.round(F.sum(F.col(\"quantity\")).over(windowSpec), 2))\n",
    "res.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+---------+--------+---------+------------+\n",
      "|      Date|customer_id|ticket_id|quantity|Net_sales|RunningTotal|\n",
      "+----------+-----------+---------+--------+---------+------------+\n",
      "|2018-01-30|        148|      347|       3|    38.18|           3|\n",
      "|2018-01-30|        148|      789|       4|    60.01|           7|\n",
      "|2018-05-05|        148|       84|       3|    66.54|           7|\n",
      "|2018-05-05|        148|      376|       3|    94.75|           6|\n",
      "|2018-05-05|        148|      574|       2|    97.67|           5|\n",
      "|2018-01-30|         31|       30|       5|    67.23|           5|\n",
      "|2018-01-30|         31|      158|       3|    37.92|           8|\n",
      "|2018-01-30|         31|      619|       3|    20.32|           6|\n",
      "|2018-01-30|         31|      778|       4|     4.86|           7|\n",
      "|2018-05-05|         31|       40|       5|    56.49|           9|\n",
      "|2018-05-05|         31|      208|       5|    68.67|          10|\n",
      "|2018-05-05|         31|      329|       1|    69.37|           6|\n",
      "|2018-05-05|         31|      581|       3|    82.17|           4|\n",
      "|2018-05-05|         31|      853|       4|    89.22|           7|\n",
      "|2018-05-05|         31|      901|       5|    79.58|           9|\n",
      "|2018-01-30|         85|      145|       1|    43.36|           1|\n",
      "|2018-01-30|         85|      811|       5|    85.18|           6|\n",
      "|2018-05-05|         85|      294|       4|    55.05|           9|\n",
      "|2018-01-30|        137|       76|       2|    63.45|           2|\n",
      "|2018-01-30|        137|      604|       2|     0.37|           4|\n",
      "+----------+-----------+---------+--------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "res.collect()[1][\"customer_id\"]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# 7. Gap/Delta in Quantity between each Customers Order\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag \n",
    "cols = [\"Date\", \"customer_id\", \"ticket_id\", \"quantity\"]\n",
    "dfff = combined_df.select(cols)\n",
    "windowSpec = Window.partitionBy([\"customer_id\"])\\\n",
    "                    .orderBy(\"Date\")\n",
    "                    \n",
    "r = dfff.withColumn(\"lag\",lag(\"quantity\", 1).over(windowSpec)) \\\n",
    "      .withColumn(\"delta\", F.round(-F.col(\"lag\")+F.col(\"quantity\")))\n",
    "      \n",
    "r.na.fill(value=0).show(100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-----------+---------+--------+---+-----+\n",
      "|      Date|customer_id|ticket_id|quantity|lag|delta|\n",
      "+----------+-----------+---------+--------+---+-----+\n",
      "|2018-01-30|        148|      347|       3|  0|    0|\n",
      "|2018-01-30|        148|      789|       4|  3|    1|\n",
      "|2018-05-05|        148|       84|       3|  4|   -1|\n",
      "|2018-05-05|        148|      376|       3|  3|    0|\n",
      "|2018-05-05|        148|      574|       2|  3|   -1|\n",
      "|2018-01-30|         31|       30|       5|  0|    0|\n",
      "|2018-01-30|         31|      158|       3|  5|   -2|\n",
      "|2018-01-30|         31|      619|       3|  3|    0|\n",
      "|2018-01-30|         31|      778|       4|  3|    1|\n",
      "|2018-05-05|         31|       40|       5|  4|    1|\n",
      "|2018-05-05|         31|      208|       5|  5|    0|\n",
      "|2018-05-05|         31|      329|       1|  5|   -4|\n",
      "|2018-05-05|         31|      581|       3|  1|    2|\n",
      "|2018-05-05|         31|      853|       4|  3|    1|\n",
      "|2018-05-05|         31|      901|       5|  4|    1|\n",
      "|2018-01-30|         85|      145|       1|  0|    0|\n",
      "|2018-01-30|         85|      811|       5|  1|    4|\n",
      "|2018-05-05|         85|      294|       4|  5|   -1|\n",
      "|2018-01-30|        137|       76|       2|  0|    0|\n",
      "|2018-01-30|        137|      604|       2|  2|    0|\n",
      "|2018-01-30|        137|      695|       1|  2|   -1|\n",
      "|2018-01-30|        137|      731|       5|  1|    4|\n",
      "|2018-05-05|        137|      647|       3|  5|   -2|\n",
      "|2018-05-05|        137|      648|       2|  3|   -1|\n",
      "|2018-05-05|        137|      972|       5|  2|    3|\n",
      "|2018-01-30|         65|      261|       1|  0|    0|\n",
      "|2018-01-30|         65|      882|       2|  1|    1|\n",
      "|2018-05-05|         65|       61|       4|  2|    2|\n",
      "|2018-01-30|         53|      454|       2|  0|    0|\n",
      "|2018-01-30|        133|      181|       2|  0|    0|\n",
      "|2018-01-30|        133|      240|       5|  2|    3|\n",
      "|2018-01-30|        133|      282|       1|  5|   -4|\n",
      "|2018-01-30|        133|      325|       5|  1|    4|\n",
      "|2018-01-30|        133|      440|       1|  5|   -4|\n",
      "|2018-01-30|        133|      597|       4|  1|    3|\n",
      "|2018-05-05|        133|      450|       1|  4|   -3|\n",
      "|2018-05-05|        133|      622|       5|  1|    4|\n",
      "|2018-05-05|        133|      705|       3|  5|   -2|\n",
      "|2018-01-30|         78|      288|       1|  0|    0|\n",
      "|2018-01-30|         78|      802|       4|  1|    3|\n",
      "|2018-01-30|         78|      832|       4|  4|    0|\n",
      "|2018-01-30|        108|       88|       4|  0|    0|\n",
      "|2018-01-30|        108|      123|       5|  4|    1|\n",
      "|2018-01-30|        108|      255|       3|  5|   -2|\n",
      "|2018-01-30|        108|      258|       4|  3|    1|\n",
      "|2018-01-30|        108|      259|       1|  4|   -3|\n",
      "|2018-01-30|        108|      323|       5|  1|    4|\n",
      "|2018-01-30|        108|      702|       3|  5|   -2|\n",
      "|2018-01-30|        108|      851|       1|  3|   -2|\n",
      "|2018-01-30|        108|      958|       5|  1|    4|\n",
      "|2018-05-05|        108|      762|       4|  5|   -1|\n",
      "|2018-05-05|        108|      777|       3|  4|   -1|\n",
      "|2018-01-30|        155|      409|       3|  0|    0|\n",
      "|2018-01-30|        155|      862|       2|  3|   -1|\n",
      "|2018-05-05|        155|      741|       5|  2|    3|\n",
      "|2018-05-05|        155|      864|       3|  5|   -2|\n",
      "|2018-01-30|         34|       31|       3|  0|    0|\n",
      "|2018-01-30|         34|      157|       1|  3|   -2|\n",
      "|2018-01-30|         34|      669|       4|  1|    3|\n",
      "|2018-01-30|         34|      737|       4|  4|    0|\n",
      "|2018-05-05|         34|      160|       2|  4|   -2|\n",
      "|2018-05-05|         34|      429|       2|  2|    0|\n",
      "|2018-05-05|         34|      898|       1|  2|   -1|\n",
      "|2018-05-05|         34|      945|       2|  1|    1|\n",
      "|2018-01-30|        193|      310|       3|  0|    0|\n",
      "|2018-01-30|        193|      350|       1|  3|   -2|\n",
      "|2018-01-30|        193|      599|       3|  1|    2|\n",
      "|2018-01-30|        193|      861|       5|  3|    2|\n",
      "|2018-01-30|        193|      870|       5|  5|    0|\n",
      "|2018-05-05|        193|      573|       4|  5|   -1|\n",
      "|2018-01-30|        101|      179|       5|  0|    0|\n",
      "|2018-01-30|        101|      481|       2|  5|   -3|\n",
      "|2018-01-30|        101|      982|       5|  2|    3|\n",
      "|2018-05-05|        101|      226|       2|  5|   -3|\n",
      "|2018-05-05|        101|      286|       4|  2|    2|\n",
      "|2018-01-30|        115|      177|       3|  0|    0|\n",
      "|2018-05-05|        115|      283|       3|  3|    0|\n",
      "|2018-05-05|        115|      801|       5|  3|    2|\n",
      "|2018-05-05|        115|      867|       5|  5|    0|\n",
      "|2018-05-05|        126|      297|       5|  0|    0|\n",
      "|2018-01-30|         81|      256|       3|  0|    0|\n",
      "|2018-05-05|         81|      144|       3|  3|    0|\n",
      "|2018-01-30|         28|      330|       3|  0|    0|\n",
      "|2018-01-30|        183|      301|       2|  0|    0|\n",
      "|2018-01-30|        183|      462|       4|  2|    2|\n",
      "|2018-01-30|        183|      915|       1|  4|   -3|\n",
      "|2018-01-30|        183|      917|       5|  1|    4|\n",
      "|2018-01-30|        183|      966|       3|  5|   -2|\n",
      "|2018-05-05|        183|       19|       1|  3|   -2|\n",
      "|2018-05-05|        183|      589|       5|  1|    4|\n",
      "|2018-05-05|        183|      947|       1|  5|   -4|\n",
      "|2018-05-05|        183|      957|       4|  1|    3|\n",
      "|2018-01-30|         76|        4|       2|  0|    0|\n",
      "|2018-01-30|         76|       70|       5|  2|    3|\n",
      "|2018-01-30|         76|      196|       2|  5|   -3|\n",
      "|2018-01-30|         76|      262|       2|  2|    0|\n",
      "|2018-01-30|         76|      298|       4|  2|    2|\n",
      "|2018-01-30|         76|      506|       5|  4|    1|\n",
      "|2018-01-30|         76|      874|       4|  5|   -1|\n",
      "|2018-05-05|         76|       18|       4|  4|    0|\n",
      "+----------+-----------+---------+--------+---+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "Tots.withColumn(\"MultiEvent\", multiEvent_udf(F.col(\"Totals\"))).show(5)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "cols = [\"customer_id\", \"event_name\", \"event_season\", \"Date\", \"rank\"]\n",
    "combined_df.groupBy(\"customer_id\").agg(F.max(\"quantity\")).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|max(quantity)|\n",
      "+-----------+-------------+\n",
      "|        148|            4|\n",
      "|         31|            5|\n",
      "|        137|            5|\n",
      "|         85|            5|\n",
      "|         65|            4|\n",
      "|         53|            2|\n",
      "|        133|            5|\n",
      "|         78|            4|\n",
      "|        108|            5|\n",
      "|        155|            5|\n",
      "|         34|            4|\n",
      "|        193|            5|\n",
      "|        115|            5|\n",
      "|        101|            5|\n",
      "|        126|            5|\n",
      "|         81|            3|\n",
      "|        183|            5|\n",
      "|         28|            3|\n",
      "|         76|            5|\n",
      "|         26|            5|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# 5. Largest Order by Quantity for each Customer\n",
    "from pyspark.sql.window import Window\n",
    "cols = [\"customer_id\", \"quantity\"]\n",
    "windowSpec1 = Window.partitionBy([\"customer_id\"])\\\n",
    "                    .orderBy(F.desc(\"quantity\"))\n",
    "\n",
    "ranked = combined_df.withColumn(\"rank\", F.rank().over(windowSpec1)).filter(\"rank == 1\").select(cols)\n",
    "ranked.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|quantity|\n",
      "+-----------+--------+\n",
      "|        148|       4|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "|         31|       5|\n",
      "+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "ranked.drop_duplicates()\\\n",
    "        .orderBy(F.desc(\"quantity\"))\\\n",
    "        .orderBy(\"customer_id\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|quantity|\n",
      "+-----------+--------+\n",
      "|          1|       3|\n",
      "|          2|       4|\n",
      "|          3|       5|\n",
      "|          4|       5|\n",
      "|          5|       4|\n",
      "|          6|       5|\n",
      "|          7|       5|\n",
      "|          8|       5|\n",
      "|          9|       5|\n",
      "|         10|       5|\n",
      "|         11|       4|\n",
      "|         12|       3|\n",
      "|         13|       4|\n",
      "|         14|       3|\n",
      "|         15|       2|\n",
      "|         16|       5|\n",
      "|         17|       4|\n",
      "|         18|       2|\n",
      "|         19|       5|\n",
      "|         20|       5|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"customer_id\").orderBy(F.desc(\"quantity\"))\n",
    "\n",
    "F = combined_df.withColumn(\"dddd\", F.collect_list(F.col(\"quantity\")).over(WindowSpec)).select(\"customer_id\", \"dddd\")\n",
    "F.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|                dddd|\n",
      "+-----------+--------------------+\n",
      "|        148|                 [4]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|        [4, 3, 3, 3]|\n",
      "|        148|     [4, 3, 3, 3, 2]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|        [5, 5, 5, 5]|\n",
      "|         31|  [5, 5, 5, 5, 4, 4]|\n",
      "|         31|  [5, 5, 5, 5, 4, 4]|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         31|[5, 5, 5, 5, 4, 4...|\n",
      "|         85|                 [5]|\n",
      "|         85|              [5, 4]|\n",
      "|         85|           [5, 4, 1]|\n",
      "|        137|              [5, 5]|\n",
      "|        137|              [5, 5]|\n",
      "+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "F.collect()[2][\"dddd\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"customer_id\")\n",
    "\n",
    "df22 = combined_df.withColumn(\"EventsList\", \n",
    "                              F.collect_list(F.col(\"event_code\"))\n",
    "                              .over(WindowSpec)\n",
    "                              )\\\n",
    ".select(\"customer_id\", \"EventsList\")\n",
    "df22.show(5, truncate=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+---------------------------------------------+\n",
      "|customer_id|EventsList                                   |\n",
      "+-----------+---------------------------------------------+\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "|148        |[CHL-ARS, BTH-SRC, CHL-ARS, CHL-ARS, F1-SILV]|\n",
      "+-----------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"event_code\", outputCol=\"IndexEvents\")\n",
    "indexed = indexer.fit(combined_df).transform(combined_df)\n",
    "indexed.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|ticket_id|order_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|IndexEvents|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|        1|     182|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             160|     Tony|   Baird|001-097-596-0629x...|   94414|           Ms.|        0.0|\n",
      "|        2|     376|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              46|   Justin| Higgins|          1889149702|   67579|          Mrs.|        0.0|\n",
      "|        3|     282|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             116| Jennifer|  Dawson|    479.687.6222x409|   23815|           Dr.|        0.0|\n",
      "|        4|      20|       2|    29.13|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              76|  Jasmine|   Bryan|        050.499.1449|   50492|          Mrs.|        1.0|\n",
      "|        5|      14|       5|     4.15|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|             190|    Larry|  Rivera|001-966-104-7383x...|   84129|           Dr.|        1.0|\n",
      "|        6|     180|       5|    66.67|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|              50|  Richard|    Wade|        867-772-7090|   38681|          Mrs.|        2.0|\n",
      "|        7|      27|       2|    63.19|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|             119|   Joseph|   Avery|          3809985291|   56074|           Dr.|        2.0|\n",
      "|        8|     328|       3|    95.11|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              35|    Allen|   Glass|001-327-121-8445x...|   34120|           Dr.|        1.0|\n",
      "|        9|     211|       5|     19.2|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              61|  Vincent| Sanchez|+1-098-920-7256x1...|   07205|           Dr.|        1.0|\n",
      "|       10|     301|       1|    13.42|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              12|  Stephen|   Ortiz|001-165-828-1070x...|   09009|          Mrs.|        0.0|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "combined_df.groupBy(\"Customer_Title\").agg(F.sum(\"quantity\").alias(\"Totals\")).sort(\"Totals\", ascending=False).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+------+\n",
      "|Customer_Title|Totals|\n",
      "+--------------+------+\n",
      "|           Dr.|  1023|\n",
      "|           Mr.|   996|\n",
      "|          Mrs.|   691|\n",
      "|          Miss|   131|\n",
      "|           Ms.|   128|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "add = lambda x,y : x+y\n",
    "add(2,3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "\n",
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "combined_df.groupBy(\"customer_id\")\\\n",
    "           .agg(F.count(\"event_code\").alias(\"Totals\"))\\\n",
    "           .sort(\"Totals\", ascending=True)\\\n",
    "           .show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+------+\n",
      "|customer_id|Totals|\n",
      "+-----------+------+\n",
      "|         86|     1|\n",
      "|        175|     1|\n",
      "|        121|     1|\n",
      "|        126|     1|\n",
      "|         28|     1|\n",
      "|         53|     1|\n",
      "|        149|     1|\n",
      "|        109|     2|\n",
      "|         61|     2|\n",
      "|         62|     2|\n",
      "|        117|     2|\n",
      "|        154|     2|\n",
      "|         24|     2|\n",
      "|        156|     2|\n",
      "|          5|     2|\n",
      "|          1|     2|\n",
      "|         15|     2|\n",
      "|        171|     2|\n",
      "|         91|     2|\n",
      "|         81|     2|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c572b0966902eb897d15fe5d929823b06d400a3cf7da46c31443f305cdaa6b64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}