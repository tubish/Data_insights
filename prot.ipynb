{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/15 18:57:39 WARN Utils: Your hostname, DESKTOP-C98VC8U resolves to a loopback address: 127.0.1.1; using 172.22.196.85 instead (on interface eth0)\n",
      "21/08/15 18:57:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/15 18:57:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/15 18:57:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "customers = spark.read.parquet(\"./output/customers.parquet\")\n",
    "customers.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|               1|    Terri| Leonard|          3124065463|   65369|          Mrs.|\n",
      "|               2|  Maurice|    West|001-468-075-5301x308|   27061|           Dr.|\n",
      "|               3|   Taylor|    Hall|    001-004-938-2651|   88302|           Mr.|\n",
      "+----------------+---------+--------+--------------------+--------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tickets = spark.read.parquet(\"./output/tickets.parquet\")\n",
    "tickets.show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|ticket_id|order_id|customer_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "|        1|     182|        160|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        2|     376|         46|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "|        3|     282|        116|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|\n",
      "+---------+--------+-----------+--------+---------+----------+------------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#broadcast joining because one table is very small, reduces shuffling\n",
    "import pyspark.sql.functions as F\n",
    "join_condition = customers.CustomerIdentity == tickets.customer_id\n",
    "combined_df =  tickets.join(F.broadcast(customers), join_condition).drop(\"customer_id\")\n",
    "combined_df.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "combined_df.printSchema()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- ticket_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- Net_sales: double (nullable = true)\n",
      " |-- event_code: string (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_season: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- CustomerIdentity: long (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LASTNAME: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- Customer_Title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "combined_df.show(1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|ticket_id|order_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+\n",
      "|        1|     182|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             160|     Tony|   Baird|001-097-596-0629x...|   94414|           Ms.|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "combined_df.select(\"Date\").where(F.col(\"Date\").isNull()).count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pyspark.sql.functions as F\n",
    "# 3. For each Customer, a list of Events\n",
    "Edf = combined_df.groupBy(\"CustomerIdentity\").agg(F.collect_list(F.col(\"event_code\")).alias(\"Events_list\"))\n",
    "Edf.show(5, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+------------------------------------+\n",
      "|CustomerIdentity|Events_list                         |\n",
      "+----------------+------------------------------------+\n",
      "|26              |[BTH-SRC, BTH-SRC, CHL-ARS, BTH-SRC]|\n",
      "|29              |[F1-SILV, CHL-ARS, F1-SILV]         |\n",
      "|65              |[F1-SILV, BTH-SRC, CHL-ARS]         |\n",
      "|191             |[CHL-ARS, F1-SILV, CHL-ARS, CHL-ARS]|\n",
      "|19              |[CHL-ARS, F1-SILV, CHL-ARS]         |\n",
      "+----------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# combined_df.groupBy(\"CustomerIdentity\").agg(F.count(\"event_code\").alias(\"Totals\")).sort(\"Totals\", ascending=True).show()\n",
    "Tots = combined_df.groupBy(\"CustomerIdentity\").agg(F.count(\"event_name\").alias(\"Totals\"))\n",
    "Tots.show(2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+------+\n",
      "|CustomerIdentity|Totals|\n",
      "+----------------+------+\n",
      "|              26|     4|\n",
      "|              29|     3|\n",
      "+----------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "MultiEvent = Tots.withColumn(\"MultiEvent\", F.col(\"Totals\") > 1)\n",
    "MultiEvent.show()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1381/4294290402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMultiEvent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MultiEvent1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Totals\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mMultiEvent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MultiEvent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MultiEvent1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def multiEvent(x):\n",
    "    \n",
    "   if x > 1:\n",
    "\n",
    "        return   \"True\"\n",
    "   else:\n",
    "        return \" \"\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "multiEvent_udf = udf(multiEvent, StringType())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "Tots.withColumn(\"MultiEvent\", multiEvent_udf(F.col(\"Totals\"))).show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+------+----------+\n",
      "|CustomerIdentity|Totals|MultiEvent|\n",
      "+----------------+------+----------+\n",
      "|              26|     4|      True|\n",
      "|              29|     3|      True|\n",
      "|              65|     3|      True|\n",
      "|             191|     4|      True|\n",
      "|              54|     8|      True|\n",
      "|              19|     3|      True|\n",
      "|             167|     3|      True|\n",
      "|             112|     5|      True|\n",
      "|             113|     3|      True|\n",
      "|             155|     4|      True|\n",
      "|             198|     5|      True|\n",
      "|              22|     3|      True|\n",
      "|             196|     6|      True|\n",
      "|             130|     3|      True|\n",
      "|              77|     9|      True|\n",
      "|               7|     4|      True|\n",
      "|              34|     8|      True|\n",
      "|             184|     3|      True|\n",
      "|             188|     6|      True|\n",
      "|             126|     1|          |\n",
      "|              50|     6|      True|\n",
      "|              94|     3|      True|\n",
      "|             149|     1|          |\n",
      "|             190|     4|      True|\n",
      "|             110|     7|      True|\n",
      "|              57|     6|      True|\n",
      "|             136|     5|      True|\n",
      "|             144|     2|      True|\n",
      "|              32|     2|      True|\n",
      "|              43|     3|      True|\n",
      "|              84|     6|      True|\n",
      "|              31|    10|      True|\n",
      "|             119|     6|      True|\n",
      "|             187|     5|      True|\n",
      "|              39|     3|      True|\n",
      "|              98|    10|      True|\n",
      "|             116|     7|      True|\n",
      "|             145|     2|      True|\n",
      "|              25|     7|      True|\n",
      "|             157|     5|      True|\n",
      "|             181|     5|      True|\n",
      "|             124|     6|      True|\n",
      "|              95|     6|      True|\n",
      "|             143|     4|      True|\n",
      "|              71|     2|      True|\n",
      "|             161|     5|      True|\n",
      "|             193|     6|      True|\n",
      "|              68|     6|      True|\n",
      "|               6|     8|      True|\n",
      "|              87|     4|      True|\n",
      "+----------------+------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "WindowSpec = Window.partitionBy(\"CustomerIdentity\")\n",
    "\n",
    "df22 = combined_df.withColumn(\"EventsList\", F.collect_list(F.col(\"event_code\")).over(WindowSpec)).select(\"CustomerIdentity\", \"EventsList\")\n",
    "df22.show(5, truncate=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+------------------------------------+\n",
      "|CustomerIdentity|EventsList                          |\n",
      "+----------------+------------------------------------+\n",
      "|26              |[BTH-SRC, BTH-SRC, BTH-SRC, CHL-ARS]|\n",
      "|26              |[BTH-SRC, BTH-SRC, BTH-SRC, CHL-ARS]|\n",
      "|26              |[BTH-SRC, BTH-SRC, BTH-SRC, CHL-ARS]|\n",
      "|26              |[BTH-SRC, BTH-SRC, BTH-SRC, CHL-ARS]|\n",
      "|29              |[F1-SILV, F1-SILV, CHL-ARS]         |\n",
      "+----------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"event_code\", outputCol=\"IndexEvents\")\n",
    "indexed = indexer.fit(combined_df).transform(combined_df)\n",
    "indexed.show(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|ticket_id|order_id|quantity|Net_sales|event_code|        event_name|event_season|      Date|CustomerIdentity|FirstName|LASTNAME|               phone|postcode|Customer_Title|IndexEvents|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "|        1|     182|       1|    58.55|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             160|     Tony|   Baird|001-097-596-0629x...|   94414|           Ms.|        0.0|\n",
      "|        2|     376|       1|    44.25|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              46|   Justin| Higgins|          1889149702|   67579|          Mrs.|        0.0|\n",
      "|        3|     282|       5|     47.0|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|             116| Jennifer|  Dawson|    479.687.6222x409|   23815|           Dr.|        0.0|\n",
      "|        4|      20|       2|    29.13|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              76|  Jasmine|   Bryan|        050.499.1449|   50492|          Mrs.|        1.0|\n",
      "|        5|      14|       5|     4.15|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|             190|    Larry|  Rivera|001-966-104-7383x...|   84129|           Dr.|        1.0|\n",
      "|        6|     180|       5|    66.67|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|              50|  Richard|    Wade|        867-772-7090|   38681|          Mrs.|        2.0|\n",
      "|        7|      27|       2|    63.19|   F1-SILV|       Silverstone|   2020/2021|2018-01-30|             119|   Joseph|   Avery|          3809985291|   56074|           Dr.|        2.0|\n",
      "|        8|     328|       3|    95.11|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              35|    Allen|   Glass|001-327-121-8445x...|   34120|           Dr.|        1.0|\n",
      "|        9|     211|       5|     19.2|   BTH-SRC|  Bath vs Saracens|   2020/2021|2018-01-30|              61|  Vincent| Sanchez|+1-098-920-7256x1...|   07205|           Dr.|        1.0|\n",
      "|       10|     301|       1|    13.42|   CHL-ARS|Chelsea vs Arsenal|   2020/2021|2018-05-05|              12|  Stephen|   Ortiz|001-165-828-1070x...|   09009|          Mrs.|        0.0|\n",
      "+---------+--------+--------+---------+----------+------------------+------------+----------+----------------+---------+--------+--------------------+--------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "combined_df.groupBy(\"Customer_Title\").agg(F.sum(\"quantity\").alias(\"Totals\")).sort(\"Totals\", ascending=False).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------+------+\n",
      "|Customer_Title|Totals|\n",
      "+--------------+------+\n",
      "|           Dr.|  1023|\n",
      "|           Mr.|   996|\n",
      "|          Mrs.|   691|\n",
      "|          Miss|   131|\n",
      "|           Ms.|   128|\n",
      "+--------------+------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#List of **all** Customers with an additional column called \"MultiEvent\", set to `True` for those Customers with more than 1 Event\n",
    "combined_df.groupBy(\"CustomerIdentity\").agg(F.count(\"event_code\").alias(\"Totals\")).sort(\"Totals\", ascending=True).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+------+\n",
      "|CustomerIdentity|Totals|\n",
      "+----------------+------+\n",
      "|              86|     1|\n",
      "|             175|     1|\n",
      "|              28|     1|\n",
      "|             121|     1|\n",
      "|             126|     1|\n",
      "|             149|     1|\n",
      "|              53|     1|\n",
      "|              61|     2|\n",
      "|              32|     2|\n",
      "|              18|     2|\n",
      "|             145|     2|\n",
      "|             156|     2|\n",
      "|             154|     2|\n",
      "|              13|     2|\n",
      "|               1|     2|\n",
      "|             123|     2|\n",
      "|              71|     2|\n",
      "|             144|     2|\n",
      "|             171|     2|\n",
      "|              66|     2|\n",
      "+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# For each Customer, a list of Events\n",
    "# combined_df.groupBy(\"CustomerIdentity\").agg(\"event_name\").select(\"CustomerIdentity\", \"event_name\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "combined_df.select(\"CustomerIdentity\").distinct().count()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "metadata": {},
     "execution_count": 20
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "combined_df.selectExpr(\"`Net_sales` > 2\").show(3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+\n",
      "|(Net_sales > 2)|\n",
      "+---------------+\n",
      "|           true|\n",
      "|           true|\n",
      "|           true|\n",
      "+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import pyspark.sql.functions as F \n",
    "event_df = indexed.groupby(\"CustomerIdentity\").agg(F.collect_list(\"IndexEvents\").alias(\"List_of_events\"))\n",
    "event_df.show(3, truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+--------------------+\n",
      "|CustomerIdentity|List_of_events      |\n",
      "+----------------+--------------------+\n",
      "|26              |[1.0, 1.0, 0.0, 1.0]|\n",
      "|29              |[2.0, 0.0, 2.0]     |\n",
      "|65              |[2.0, 1.0, 0.0]     |\n",
      "+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c572b0966902eb897d15fe5d929823b06d400a3cf7da46c31443f305cdaa6b64"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}